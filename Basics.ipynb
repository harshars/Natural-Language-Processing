{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String Operations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word in lowercase:  python\n",
      "\n",
      "Word capitalized:  Python\n",
      "\n",
      "Word in uppercase:  PYTHON\n",
      "\n",
      "Word with case swapped:  pYtHON\n"
     ]
    }
   ],
   "source": [
    "name = 'PyThon'\n",
    "print ('Word in lowercase: ',name.lower())\n",
    "print ('\\nWord capitalized: ',name.capitalize())\n",
    "print ('\\nWord in uppercase: ',name.upper())\n",
    "print ('\\nWord with case swapped: ',name.swapcase())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Italy'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Replacing parts of a string with something else\n",
    "name = 'Ptaly'\n",
    "name = name.replace('P','I')\n",
    "name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Unites States Of America', 'Unites states of america')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'unites states of america'.title() , 'unites states of america'.capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First character: I\n",
      "First three charcters : Ind\n"
     ]
    }
   ],
   "source": [
    "#Indexing and Slicing\n",
    "a = 'India'\n",
    "print ('First character:',a[0])\n",
    "print ('First three charcters :' , a[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Definitions\n",
    "\n",
    "#### Corpus    : Entire text we have\n",
    "\n",
    "#### Vocabulary : Set of all unique words in the corpus \n",
    "\n",
    "#### Document  : Unit of a corpus \n",
    "\n",
    "#### Token     : Unit of a document\n",
    "\n",
    "#### Stopwords : Words which is not needed in the business context - #####Eg: a, an , the etc\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Text Preprocessing\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = 'Paris is the capital city of France. Eiffel Tower is the biggest tourist attraction there. \\\n",
    "I have been there and I love the city. Mr. Obama is '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Paris is the capital city of France.',\n",
       " 'Eiffel Tower is the biggest tourist attraction there.',\n",
       " 'I have been there and I love the city.',\n",
       " 'Mr. Obama is']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Paris', 'is', 'the', 'capital', 'city', 'of', 'France', '.'], ['Eiffel', 'Tower', 'is', 'the', 'biggest', 'tourist', 'attraction', 'there', '.'], ['I', 'have', 'been', 'there', 'and', 'I', 'love', 'the', 'city', '.'], ['Mr.', 'Obama', 'is']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize , RegexpTokenizer\n",
    "print([word_tokenize(x) for x in sent_tokenize(text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "\n",
    "Common words that can be expected in a large number in any document and which might not add a lot of value for \\\n",
    "performing many analyses.\n",
    "\n",
    "Common Stop Words : 'I' , 'You' , 'The' ,'is' etc. - Can be accessed through nlkt stopwords\n",
    "\n",
    "Contextual Stop Words : Based on the context . For example , in a corpus containing people reviews about Disneyland\\\n",
    "will have Disneyland occuring everywhere.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords_list = stopwords.words('english')\n",
    "print(stopwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print([x for x in stopwords_list if x not in ['i']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use Python list operations to add more words to the list based on the context.Say we want to add Disney\\\n",
    "and Disneyland in the list , we can just create a list and add it to the existing stopwords list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'Disneyland', 'Disney']\n"
     ]
    }
   ],
   "source": [
    "words_to_add = ['Disneyland' , 'Disney']\n",
    "stopwords_list = stopwords_list + words_to_add\n",
    "print(stopwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens_raw = word_tokenize(text)\n",
    "tokens = [x.lower() for x in tokens_raw if x.lower() not in stopwords_list and len(x)>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Paris', 'is', 'the', 'capital', 'city', 'of', 'France', '.', 'Eiffel', 'Tower', 'is', 'the', 'biggest', 'tourist', 'attraction', 'there', '.', 'I', 'have', 'been', 'there', 'and', 'I', 'love', 'the', 'city', '.', 'Mr.', 'Obama', 'is']\n"
     ]
    }
   ],
   "source": [
    "print(tokens_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['paris', 'capital', 'city', 'france', 'eiffel', 'tower', 'biggest', 'tourist', 'attraction', 'love', 'city', 'mr.', 'obama']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 30)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens) , len(tokens_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parts of Speech tagging\n",
    "Working with words demands knowledge of the POS they belong to. \n",
    "\n",
    "We won't always need all the token from text. Sometimes we might just work with Nouns or Adjectives\n",
    "\n",
    "So , POS tagging is an important task of NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('John', 'NNP'),\n",
       " ('was', 'VBD'),\n",
       " ('going', 'VBG'),\n",
       " ('to', 'TO'),\n",
       " ('Germany', 'NNP'),\n",
       " ('with', 'IN'),\n",
       " ('Jack', 'NNP')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'John was going to Germany with Jack'\n",
    "nltk.pos_tag(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    Tag    Description\n",
    "1.\tCC\tCoordinating conjunction\n",
    "2.\tCD\tCardinal number\n",
    "3.\tDT\tDeterminer\n",
    "4.\tEX\tExistential there\n",
    "5.\tFW\tForeign word\n",
    "6.\tIN\tPreposition or subordinating conjunction\n",
    "7.\tJJ\tAdjective\n",
    "8.\tJJR\tAdjective, comparative\n",
    "9.\tJJS\tAdjective, superlative\n",
    "10.\tLS\tList item marker\n",
    "11.\tMD\tModal\n",
    "12.\tNN\tNoun, singular or mass\n",
    "13.\tNNS\tNoun, plural\n",
    "14.\tNNP\tProper noun, singular\n",
    "15.\tNNPS\tProper noun, plural\n",
    "16.\tPDT\tPredeterminer\n",
    "17.\tPOS\tPossessive ending\n",
    "18.\tPRP\tPersonal pronoun\n",
    "19.\tPRP$\tPossessive pronoun\n",
    "20.\tRB\tAdverb\n",
    "21.\tRBR\tAdverb, comparative\n",
    "22.\tRBS\tAdverb, superlative\n",
    "23.\tRP\tParticle\n",
    "24.\tSYM\tSymbol\n",
    "25.\tTO\tto\n",
    "26.\tUH\tInterjection\n",
    "27.\tVB\tVerb, base form\n",
    "28.\tVBD\tVerb, past tense\n",
    "29.\tVBG\tVerb, gerund or present participle\n",
    "30.\tVBN\tVerb, past participle\n",
    "31.\tVBP\tVerb, non-3rd person singular present\n",
    "32.\tVBZ\tVerb, 3rd person singular present\n",
    "33.\tWDT\tWh-determiner\n",
    "34.\tWP\tWh-pronoun\n",
    "35.\tWP$\tPossessive wh-pronoun\n",
    "36.\tWRB\tWh-adverb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_tags = nltk.pos_tag(word_tokenize(text))\n",
    "filtered_words = [x[0] for x in pos_tags if x[1]=='NNP'] #Filter only proper nouns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['John', 'Germany', 'Jack']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['John', 'was', 'Germany', 'Jack']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_words = [x[0] for x in pos_tags if x[1] in ['NNP','VBD']]\n",
    "filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = 'Donald Trump is the President of the United States. He was a surprise winner in the election.\\\n",
    "He was elected the President after he defeated Hillary Clinton in the process'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Donald', 'NNP'),\n",
       " ('Trump', 'NNP'),\n",
       " ('is', 'VBZ'),\n",
       " ('the', 'DT'),\n",
       " ('President', 'NNP'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('United', 'NNP'),\n",
       " ('States', 'NNPS'),\n",
       " ('.', '.'),\n",
       " ('He', 'PRP'),\n",
       " ('was', 'VBD'),\n",
       " ('a', 'DT'),\n",
       " ('surprise', 'NN'),\n",
       " ('winner', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('election.He', 'NN'),\n",
       " ('was', 'VBD'),\n",
       " ('elected', 'VBN'),\n",
       " ('the', 'DT'),\n",
       " ('President', 'NNP'),\n",
       " ('after', 'IN'),\n",
       " ('he', 'PRP'),\n",
       " ('defeated', 'VBD'),\n",
       " ('Hillary', 'NNP'),\n",
       " ('Clinton', 'NNP'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('process', 'NN')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(nltk.word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization\n",
    "\n",
    "Different forms of the same word don't add much value to the vocabulary.\n",
    "\n",
    "It is important for to feed the algorithms only words that are different.\n",
    "\n",
    "For example 'running','runs','ran','run' are all basically the same word that is run\n",
    "\n",
    "The process of obtaining the base form of a word is what is expected from a Stemming/Lemmatization Algorithm\n",
    "\n",
    "However there are differences in the way stemming and lemmatization perform the task.\n",
    "\n",
    "Stemming just cuts off the last few characters of a word based on the patterns which has been defined previously. \n",
    "\n",
    "Lemmatization takes into account the morphology of the word and does the inflection removal accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car  :  car\n",
      "cars  :  car\n",
      "thief  :  thief\n",
      "thieves  :  thiev\n",
      "computer  :  comput\n",
      "computing  :  comput\n",
      "advertisement  :  advertis\n",
      "advertisements  :  advertis\n"
     ]
    }
   ],
   "source": [
    "word_list = [ 'car','cars','thief','thieves' , 'computer' , 'computing' , 'advertisement','advertisements']\n",
    "for word in word_list:\n",
    "    print (word , ' : ' ,stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car  :  car\n",
      "cars  :  car\n",
      "thief  :  thief\n",
      "thieves  :  thief\n",
      "computer  :  computer\n",
      "computing  :  computing\n",
      "advertisement  :  advertisement\n",
      "advertisements  :  advertisement\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for word in word_list:\n",
    "    print (word , ' : ' ,lemmatizer.lemmatize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "am  :  am\n",
      "is  :  is\n",
      "are  :  are\n"
     ]
    }
   ],
   "source": [
    "another_word_list = ['am' , 'is' , 'are']\n",
    "for word in another_word_list:\n",
    "    print (word , ' : ' ,stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "am  :  am\n",
      "is  :  is\n",
      "are  :  are\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "for word in another_word_list:\n",
    "    print (word , ' : ' ,lemmatizer.lemmatize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'compute'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('computing',wordnet.VERB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = 'Advertisements are the main source of income for many bloggers. Every blogger accepts some terms and \\\n",
    "conditions if they want to earn through an advertisement. Failing to accept any condition leads to failure of \\\n",
    "the contract'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = word_tokenize(text.lower())\n",
    "\n",
    "unique_tokens = list(set(tokens))\n",
    "\n",
    "stemmed_tokens = [stemmer.stem(x.lower()) for x in unique_tokens]\n",
    "\n",
    "unique_stemmed_tokens = list(set(stemmed_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no of unique tokens:  33\n",
      "no of unique stemmed tokens:  29\n"
     ]
    }
   ],
   "source": [
    "print('no of unique tokens: ', len(unique_tokens))\n",
    "print('no of unique stemmed tokens: ', len(unique_stemmed_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'machin'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('machine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('running', 'run')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('running', wordnet.NOUN), lemmatizer.lemmatize('running', wordnet.VERB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comput\n",
      "computing\n",
      "compute\n"
     ]
    }
   ],
   "source": [
    "print (stemmer.stem('computing'))\n",
    "print (lemmatizer.lemmatize('computing'))\n",
    "print (lemmatizer.lemmatize('computing',get_wordnet_pos('VBG')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This lemma is  This\n",
      "is lemma is be\n",
      "a lemma is  a\n",
      "bad lemma is bad\n",
      "place lemma is place\n",
      "in lemma is  in\n",
      "the lemma is  the\n",
      "better lemma is good\n",
      "places lemma is place\n"
     ]
    }
   ],
   "source": [
    "words = ['This' , 'is' , 'a' , 'bad' ,'place','in','the','better','places']\n",
    "import nltk \n",
    "for word , tag in nltk.pos_tag(words):\n",
    "    if get_wordnet_pos(tag):\n",
    "        print (word,'lemma is' , lemmatizer.lemmatize(word , get_wordnet_pos(tag )))\n",
    "    else:\n",
    "        print (word , 'lemma is ',lemmatizer.lemmatize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'running'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('running')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'administr'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('administration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('running',get_wordnet_pos('VBG'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-gram modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I', 'need', 'to', 'write')\n",
      "('need', 'to', 'write', 'a')\n",
      "('to', 'write', 'a', 'program')\n",
      "('write', 'a', 'program', 'in')\n",
      "('a', 'program', 'in', 'NLTK')\n",
      "('program', 'in', 'NLTK', 'that')\n",
      "('in', 'NLTK', 'that', 'breaks')\n",
      "('NLTK', 'that', 'breaks', 'a')\n",
      "('that', 'breaks', 'a', 'corpus')\n",
      "('breaks', 'a', 'corpus', '(')\n",
      "('a', 'corpus', '(', 'a')\n",
      "('corpus', '(', 'a', 'large')\n",
      "('(', 'a', 'large', 'collection')\n",
      "('a', 'large', 'collection', 'of')\n",
      "('large', 'collection', 'of', 'txt')\n",
      "('collection', 'of', 'txt', 'files')\n",
      "('of', 'txt', 'files', ')')\n",
      "('txt', 'files', ')', 'into')\n",
      "('files', ')', 'into', 'unigrams')\n",
      "(')', 'into', 'unigrams', ',')\n",
      "('into', 'unigrams', ',', 'bigrams')\n",
      "('unigrams', ',', 'bigrams', ',')\n",
      "(',', 'bigrams', ',', 'trigrams')\n",
      "('bigrams', ',', 'trigrams', ',')\n",
      "(',', 'trigrams', ',', 'fourgrams')\n",
      "('trigrams', ',', 'fourgrams', 'and')\n",
      "(',', 'fourgrams', 'and', 'fivegrams.I')\n",
      "('fourgrams', 'and', 'fivegrams.I', 'need')\n",
      "('and', 'fivegrams.I', 'need', 'to')\n",
      "('fivegrams.I', 'need', 'to', 'write')\n",
      "('need', 'to', 'write', 'a')\n",
      "('to', 'write', 'a', 'program')\n",
      "('write', 'a', 'program', 'in')\n",
      "('a', 'program', 'in', 'NLTK')\n",
      "('program', 'in', 'NLTK', 'that')\n",
      "('in', 'NLTK', 'that', 'breaks')\n",
      "('NLTK', 'that', 'breaks', 'a')\n",
      "('that', 'breaks', 'a', 'corpus')\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "text = \"I need to write a program in NLTK that breaks a corpus (a large collection of \\\n",
    "txt files) into unigrams, bigrams, trigrams, fourgrams and fivegrams.I need to write a program in NLTK that breaks a corpus\"\n",
    "token = nltk.word_tokenize(text)\n",
    "bigrams = ngrams(token,2)\n",
    "trigrams = ngrams(token,3)\n",
    "fourgrams = ngrams(token,4)\n",
    "fivegrams = ngrams(token,5)\n",
    "\n",
    "for i in fourgrams:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = '''Andrew Yan-Tak Ng is a Chinese American computer scientist.\n",
    "He is the former chief scientist at Baidu, where he led the company's\n",
    "Artificial Intelligence Group. He is an adjunct professor (formerly \n",
    "associate professor) at Stanford University. Ng is also the co-founder\n",
    "and chairman at Coursera, an online education platform. Andrew was born\n",
    "in the UK in 1976. His parents were both from Hong Kong.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize doc\n",
    "tokenized_doc = nltk.word_tokenize(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tag sentences and use nltk's Named Entity Chunker\n",
    "tagged_sentences = nltk.pos_tag(tokenized_doc)\n",
    "ne_chunked_sents = nltk.ne_chunk(tagged_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Andrew/NNP)\n",
      "  Yan-Tak/NNP\n",
      "  Ng/NNP\n",
      "  is/VBZ\n",
      "  a/DT\n",
      "  (GPE Chinese/JJ)\n",
      "  (GPE American/JJ)\n",
      "  computer/NN\n",
      "  scientist/NN\n",
      "  ./.\n",
      "  He/PRP\n",
      "  is/VBZ\n",
      "  the/DT\n",
      "  former/JJ\n",
      "  chief/JJ\n",
      "  scientist/NN\n",
      "  at/IN\n",
      "  (ORGANIZATION Baidu/NNP)\n",
      "  ,/,\n",
      "  where/WRB\n",
      "  he/PRP\n",
      "  led/VBD\n",
      "  the/DT\n",
      "  (ORGANIZATION\n",
      "    company's/NN\n",
      "    Artificial/NNP\n",
      "    Intelligence/NNP\n",
      "    Group/NNP)\n",
      "  ./.\n",
      "  He/PRP\n",
      "  is/VBZ\n",
      "  an/DT\n",
      "  adjunct/NN\n",
      "  professor/NN\n",
      "  (/(\n",
      "  formerly/RB\n",
      "  associate/NN\n",
      "  professor/NN\n",
      "  )/)\n",
      "  at/IN\n",
      "  (ORGANIZATION Stanford/NNP University/NNP)\n",
      "  ./.\n",
      "  Ng/NNP\n",
      "  is/VBZ\n",
      "  also/RB\n",
      "  the/DT\n",
      "  co-founder/NN\n",
      "  and/CC\n",
      "  chairman/NN\n",
      "  at/IN\n",
      "  (ORGANIZATION Coursera/NNP)\n",
      "  ,/,\n",
      "  an/DT\n",
      "  online/JJ\n",
      "  education/NN\n",
      "  platform/NN\n",
      "  ./.\n",
      "  (PERSON Andrew/NNP)\n",
      "  was/VBD\n",
      "  born/VBN\n",
      "  in/IN\n",
      "  the/DT\n",
      "  UK/NNP\n",
      "  in/IN\n",
      "  1976/CD\n",
      "  ./.\n",
      "  His/PRP$\n",
      "  parents/NNS\n",
      "  were/VBD\n",
      "  both/DT\n",
      "  from/IN\n",
      "  (GPE Hong/NNP Kong/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "print(ne_chunked_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Andrew', 'PERSON'), ('Chinese', 'GPE'), ('American', 'GPE'), ('Baidu', 'ORGANIZATION'), (\"company's Artificial Intelligence Group\", 'ORGANIZATION'), ('Stanford University', 'ORGANIZATION'), ('Coursera', 'ORGANIZATION'), ('Andrew', 'PERSON'), ('Hong Kong', 'GPE')]\n"
     ]
    }
   ],
   "source": [
    "# extract all named entities\n",
    "named_entities = []\n",
    "for tagged_tree in ne_chunked_sents:\n",
    "    if hasattr(tagged_tree, 'label'):\n",
    "        entity_name = ' '.join(c[0] for c in tagged_tree.leaves()) #\n",
    "        entity_type = tagged_tree.label() # get NE category\n",
    "        named_entities.append((entity_name, entity_type))\n",
    "print(named_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name Entity: (Andrew Yan-Tak Ng, Chinese, American, Baidu, \n",
      "Artificial Intelligence Group, Stanford University, Coursera, Andrew, UK, 1976, Hong Kong)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en') # install 'en' model (python3 -m spacy download en)\n",
    "doc = nlp(doc)\n",
    "print('Name Entity: {0}'.format(doc.ents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Andrew Yan-Tak Ng - PERSON\n",
      "Chinese - NORP\n",
      "American - NORP\n",
      "Baidu - GPE\n",
      "\n",
      "Artificial Intelligence Group - ORG\n",
      "Stanford University - ORG\n",
      "Coursera - ORG\n",
      "Andrew - PERSON\n",
      "UK - GPE\n",
      "1976 - DATE\n",
      "Hong Kong - GPE\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, \"-\", ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spell Corrector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pattern.en import suggest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"amzzing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('amazing', 1.0)]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suggest(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
